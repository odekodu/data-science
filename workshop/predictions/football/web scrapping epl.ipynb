{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://fbref.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_list = [\n",
    "    { \n",
    "        'authority': 'fbref.com', \n",
    "        'cache-control': 'max-age=0', \n",
    "        'sec-ch-ua': '\"Chromium\";v=\"92\", \" Not A;Brand\";v=\"99\", \"Google Chrome\";v=\"92\"', \n",
    "        'sec-ch-ua-mobile': '?0', \n",
    "        'upgrade-insecure-requests': '1', \n",
    "        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', \n",
    "        'sec-fetch-site': 'none', \n",
    "        'sec-fetch-mode': 'navigate', \n",
    "        'sec-fetch-user': '?1', \n",
    "        'sec-fetch-dest': 'document', \n",
    "        'sec-ch-ua-platform': 'macOS',\n",
    "        'accept-language': 'en-US,en;q=0.9', \n",
    "    },\n",
    "    { \n",
    "        'authority': 'fbref.com', \n",
    "        'cache-control': 'max-age=0', \n",
    "        'sec-ch-ua': '\"Chromium\";v=\"128\", \"Not;A=Brand\";v=\"24\", \"Brave\";v=\"128\"', \n",
    "        'sec-ch-ua-mobile': '?0', \n",
    "        'upgrade-insecure-requests': '1', \n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36', \n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', \n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-mode': 'navigate', \n",
    "        'sec-fetch-user': '?1', \n",
    "        'sec-fetch-dest': 'document', \n",
    "        'sec-ch-ua-platform': 'macOS',\n",
    "        'accept-language': 'en-US,en;q=0.9', \n",
    "    }\n",
    "] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    \"\"\"\n",
    "    Sends a GET request to the url and returns the response.\n",
    "    Includes a random timeout between 2 and 20 seconds to avoid crawlting rate limits.\n",
    "    If the server responds with a 429 error, raises an exception\n",
    "    \"\"\"\n",
    "    timeout = random.randint(2, 20)\n",
    "    time.sleep(timeout)\n",
    "    print('sleeping for ', timeout)\n",
    "\n",
    "    # Choose a random set of headers to include in the request\n",
    "    headers = random.choice(headers_list)\n",
    "    \n",
    "    # Send the request\n",
    "    data = requests.get(url, headers=headers)\n",
    "    \n",
    "    # If the server responds with a 429 error, raise an exception\n",
    "    if 'Rate Limited Request (429 error)' in data.text:\n",
    "        raise Exception('Rate limit error')\n",
    "    \n",
    "    # Return the response\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_teams_url(url) :\n",
    "    data = crawl(url)\n",
    "    soup = BeautifulSoup(data.text)\n",
    "    standings_table = soup.select('table.stats_table')[0]\n",
    "    \n",
    "    links = standings_table.findAll('a')\n",
    "    links = [l.get('href') for l in links if '/squads/' in l.get('href')]\n",
    "    teams_url = [f'{base_url}{l}' for l in links]\n",
    "    \n",
    "    return teams_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_data(url):\n",
    "    data = crawl(url)\n",
    "    matches = pd.read_html(StringIO(data.text), match= 'Scores & Fixtures')[0]\n",
    "    \n",
    "    soup = BeautifulSoup(data.text)\n",
    "    links = soup.findAll('a')\n",
    "    links = [l.get('href') for l in links]\n",
    "    links = [f'{base_url}/{l}' for l in links if l is not None and '/all_comps/shooting' in l]\n",
    "    \n",
    "    data = crawl(links[0])\n",
    "    shooting = pd.read_html(StringIO(data.text), match= 'Shooting')[0]\n",
    "    shooting.columns = shooting.columns.droplevel()\n",
    "    team_data = matches.merge(shooting[['Date', 'Sh', 'SoT', 'PK', 'FK', 'PKatt', 'Dist']], on= 'Date')\n",
    "\n",
    "    return team_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2022, 2020, -1))\n",
    "all_matches = []\n",
    "\n",
    "for year in years:\n",
    "    season = f'{year - 1}-{year}'\n",
    "    standing_url = f'{base_url}/en/comps/9/{season}/{season}-Premier-League-Stats'\n",
    "    teams_url = get_teams_url(standing_url)\n",
    "\n",
    "    for team in teams_url:\n",
    "        team_name = team.split('/')[-1].replace('Stats', '').replace('-', ' ')\n",
    "\n",
    "        try:\n",
    "            team_data = get_team_data(team)\n",
    "            team_data = team_data[team_data['Comp'] == 'Premier League']\n",
    "            team_data['Name'] = team_name\n",
    "            team_data['Season'] = year\n",
    "\n",
    "            all_matches.append(team_data)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "match_df = pd.concat(all_matches)\n",
    "match_df.columns = [c.lower() for c in match_df.columns]\n",
    "\n",
    "match_df.to_csv('matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = crawl(base_url)\n",
    "data.text\n",
    "data.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
